{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from queue import PriorityQueue\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from queue import LifoQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dusan Tadic  - Southampton -  won a free kick on the left wing.', 'Missed chance. Dusan Tadic  - Southampton -  shot with left foot from the centre of the box missed to the left.', 'Dusan Tadic  - Southampton -  won a free kick in defence.', 'Fouled by Dusan Tadic  - Southampton', 'Offside - Southampton. Dusan Tadic with a pass, however Shane Long is in offside.', 'Missed chance. Dusan Tadic  - Southampton -  shot with left foot from the centre of the box missed. Assist -  Shane Long.', 'Missed chance. Dusan Tadic  - Southampton -  shot with left foot from outside the box is high and wide to the left after corner.', 'Fouled by Dusan Tadic  - Southampton', 'Dusan Tadic  - Southampton -  won a free kick on the left wing.', 'New attacking attempt. Charlie Austin  - Southampton -  shot with right foot from outside the box is saved by goalkeeper in the centre of the goal. Assist -  Dusan Tadic.']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('commentary.xlsx')\n",
    "commentary = df['commentary'].tolist()\n",
    "print(commentary[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dusan', 'Tadic', 'Southampton', 'won', 'a', 'free', 'kick', 'on', 'the', 'left', 'wing', 'Missed', 'chance', 'Dusan', 'Tadic', 'Southampton', 'shot', 'with', 'left', 'foot', 'from', 'the', 'centre', 'of', 'the', 'box', 'missed', 'to', 'the', 'left', 'Dusan', 'Tadic', 'Southampton', 'won', 'a', 'free', 'kick', 'in', 'defence', 'Fouled', 'by', 'Dusan', 'Tadic', 'Southampton', 'Offside', 'Southampton', 'Dusan', 'Tadic', 'with', 'a', 'pass', 'however', 'Shane', 'Long', 'is', 'in', 'offside', 'Missed', 'chance', 'Dusan', 'Tadic', 'Southampton', 'shot', 'with', 'left', 'foot', 'from', 'the', 'centre', 'of', 'the', 'box', 'missed', 'Assist', 'Shane', 'Long', 'Missed', 'chance', 'Dusan', 'Tadic', 'Southampton', 'shot', 'with', 'left', 'foot', 'from', 'outside', 'the', 'box', 'is', 'high', 'and', 'wide', 'to', 'the', 'left', 'after', 'corner', 'Fouled', 'by', 'Dusan', 'Tadic', 'Southampton', 'Dusan', 'Tadic', 'Southampton', 'won', 'a', 'free', 'kick', 'on', 'the', 'left', 'wing', 'New', 'attacking', 'attempt', 'Charlie', 'Austin', 'Southampton', 'shot', 'with', 'right', 'foot', 'from', 'outside', 'the', 'box', 'is', 'saved', 'by', 'goalkeeper', 'in', 'the', 'centre', 'of', 'the', 'goal', 'Assist', 'Dusan', 'Tadic']\n"
     ]
    }
   ],
   "source": [
    "tokens = [word_tokenize(comment) for comment in commentary]\n",
    "# tokens = [word for word in word_tokenize(\" \".join(commentary))]\n",
    "tokens = [[word.replace(\"'\", \"\") for word in comment] for comment in tokens]\n",
    "punc = [',', '-', '.', \"'\", '[', ']', '', ' ', '(', ')', '!']\n",
    "tokens = [[word for word in sentence if word not in punc and not word.isdigit()] for sentence in tokens]\n",
    "print(tokens[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Corner\tPOS Tag: NNP\n",
      "Token: Leicester\tPOS Tag: NNP\n",
      "Token: City\tPOS Tag: NNP\n",
      "Token: Conceded\tPOS Tag: NNP\n",
      "Token: by\tPOS Tag: IN\n",
      "Token: Jan\tPOS Tag: NNP\n",
      "Token: Bednarek\tPOS Tag: NNP\n",
      "Token: Fouled\tPOS Tag: NNP\n",
      "Token: by\tPOS Tag: IN\n",
      "Token: Jan\tPOS Tag: NNP\n"
     ]
    }
   ],
   "source": [
    "# POS tagging\n",
    "pos_tags = [pos_tag(word) for word in tokens]\n",
    "count = 0 \n",
    "for token, pos in pos_tags[2]:\n",
    "    print(f\"Token: {token}\\tPOS Tag: {pos}\")\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Dusan/NNP)\n",
      "  (PERSON Tadic/NNP Southampton/NNP)\n",
      "  won/VBD\n",
      "  a/DT\n",
      "  free/JJ\n",
      "  kick/NN\n",
      "  on/IN\n",
      "  the/DT\n",
      "  left/NN\n",
      "  wing/NN\n",
      "  Missed/VBD\n",
      "  chance/NN\n",
      "  (PERSON Dusan/NNP Tadic/NNP Southampton/NNP)\n",
      "  shot/NN\n",
      "  with/IN\n",
      "  left/JJ\n",
      "  foot/NN\n",
      "  from/IN\n",
      "  the/DT\n",
      "  centre/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  box/NN\n",
      "  missed/VBD\n",
      "  to/TO\n",
      "  the/DT\n",
      "  left/VBN\n",
      "  (PERSON Dusan/NNP Tadic/NNP Southampton/NNP)\n",
      "  won/VBD\n",
      "  a/DT\n",
      "  free/JJ\n",
      "  kick/NN\n",
      "  in/IN\n",
      "  defence/NN\n",
      "  Fouled/VBN\n",
      "  by/IN\n",
      "  (PERSON\n",
      "    Dusan/NNP\n",
      "    Tadic/NNP\n",
      "    Southampton/NNP\n",
      "    Offside/NNP\n",
      "    Southampton/NNP\n",
      "    Dusan/NNP\n",
      "    Tadic/NNP)\n",
      "  with/IN\n",
      "  a/DT\n",
      "  pass/NN\n",
      "  however/RB\n",
      "  (PERSON Shane/NNP Long/NNP)\n",
      "  is/VBZ\n",
      "  in/IN\n",
      "  offside/NN\n",
      "  Missed/VBD\n",
      "  chance/NN\n",
      "  (PERSON Dusan/NNP Tadic/NNP Southampton/NNP)\n",
      "  shot/NN\n",
      "  with/IN\n",
      "  left/JJ\n",
      "  foot/NN\n",
      "  from/IN\n",
      "  the/DT\n",
      "  centre/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  box/NN\n",
      "  missed/VBD\n",
      "  (PERSON Assist/NNP Shane/NNP Long/NNP)\n",
      "  Missed/VBD\n",
      "  chance/NN\n",
      "  (PERSON Dusan/NNP Tadic/NNP Southampton/NNP)\n",
      "  shot/NN\n",
      "  with/IN\n",
      "  left/JJ\n",
      "  foot/NN\n",
      "  from/IN\n",
      "  outside/IN\n",
      "  the/DT\n",
      "  box/NN\n",
      "  is/VBZ\n",
      "  high/JJ\n",
      "  and/CC\n",
      "  wide/JJ\n",
      "  to/TO\n",
      "  the/DT\n",
      "  left/NN\n",
      "  after/IN\n",
      "  corner/NN\n",
      "  Fouled/VBN\n",
      "  by/IN\n",
      "  (PERSON\n",
      "    Dusan/NNP\n",
      "    Tadic/NNP\n",
      "    Southampton/NNP\n",
      "    Dusan/NNP\n",
      "    Tadic/NNP\n",
      "    Southampton/NNP)\n",
      "  won/VBD\n",
      "  a/DT\n",
      "  free/JJ\n",
      "  kick/NN\n",
      "  on/IN\n",
      "  the/DT\n",
      "  left/NN\n",
      "  wing/VBG\n",
      "  (GPE New/NNP)\n",
      "  attacking/VBG\n",
      "  attempt/NN\n",
      "  (PERSON Charlie/NNP Austin/NNP Southampton/NNP)\n",
      "  shot/NN\n",
      "  with/IN\n",
      "  right/JJ\n",
      "  foot/NN\n",
      "  from/IN\n",
      "  outside/IN\n",
      "  the/DT\n",
      "  box/NN\n",
      "  is/VBZ\n",
      "  saved/VBN\n",
      "  by/IN\n",
      "  goalkeeper/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  centre/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  goal/NN\n",
      "  (PERSON Assist/NNP Dusan/NNP Tadic/NNP))\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition\n",
    "ner_tags = [ne_chunk(word) for word in pos_tags]\n",
    "print(ner_tags[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dusan', 'Tadic', 'Southampton', 'win', 'a', 'free', 'kick', 'on', 'the', 'left', 'wing', 'miss', 'chance', 'Dusan', 'Tadic', 'Southampton', 'shot', 'with', 'left', 'foot', 'from', 'the', 'centre', 'of', 'the', 'box', 'miss', 'to', 'the', 'leave', 'Dusan', 'Tadic', 'Southampton', 'win', 'a', 'free', 'kick', 'in', 'defence', 'foul', 'by', 'Dusan', 'Tadic', 'Southampton', 'Offside', 'Southampton', 'Dusan', 'Tadic', 'with', 'a', 'pass', 'however', 'Shane', 'Long', 'be', 'in', 'offside', 'miss', 'chance', 'Dusan', 'Tadic', 'Southampton', 'shot', 'with', 'left', 'foot', 'from', 'the', 'centre', 'of', 'the', 'box', 'miss', 'Assist', 'Shane', 'Long', 'miss', 'chance', 'Dusan', 'Tadic', 'Southampton', 'shot', 'with', 'left', 'foot', 'from', 'outside', 'the', 'box', 'be', 'high', 'and', 'wide', 'to', 'the', 'left', 'after', 'corner', 'foul', 'by', 'Dusan', 'Tadic', 'Southampton', 'Dusan', 'Tadic', 'Southampton', 'win', 'a', 'free', 'kick', 'on', 'the', 'left', 'wing', 'New', 'attack', 'attempt', 'Charlie', 'Austin', 'Southampton', 'shot', 'with', 'right', 'foot', 'from', 'outside', 'the', 'box', 'be', 'save', 'by', 'goalkeeper', 'in', 'the', 'centre', 'of', 'the', 'goal', 'Assist', 'Dusan', 'Tadic']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lowercase_pos_tags = [[(token.lower(), pos) if pos != 'NNP' else (token, pos) for token, pos in pos_tag] for pos_tag in pos_tags]\n",
    "union = []\n",
    "for pos_tag in lowercase_pos_tags:\n",
    "    lemmas_words = []\n",
    "    for token, pos in pos_tag:\n",
    "        if pos != 'NNP':\n",
    "            wn_pos = get_wordnet_pos(pos)\n",
    "            if wn_pos is not None:\n",
    "                if token.lower() == 'pass':\n",
    "                    lemmas_words.append(token)\n",
    "                else: \n",
    "                    lemma = wnl.lemmatize(token, pos=wn_pos)\n",
    "                    lemmas_words.append(lemma)\n",
    "            else:\n",
    "                lemmas_words.append(token)\n",
    "        else:\n",
    "            lemmas_words.append(token)\n",
    "    union.append(lemmas_words)\n",
    "\n",
    "print(union[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Corner', 'Leicester', 'City', 'Conceded', 'Jan', 'Bednarek', 'Fouled', 'Jan', 'Bednarek', 'Southampton']\n"
     ]
    }
   ],
   "source": [
    "# remove stop words\n",
    "stop_word = stopwords.words('english')\n",
    "stop_word.remove('won')\n",
    "commentary_filtered = [[word for word in words if word not in stop_word] for words in union]\n",
    "print(commentary_filtered[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"D:\\\\google_model\\\\GoogleNews-vectors-negative300.bin.gz\"\n",
    "# model = KeyedVectors.load_word2vec_format(path, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Word2Vec model object and train \n",
    "model = Word2Vec(commentary_filtered, vector_size=100, window=5, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words count: 982\n",
      "win\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "for sublist in commentary_filtered:\n",
    "    unique_words.update(sublist)\n",
    "count = len(unique_words)\n",
    "print(\"Unique words count:\", count)\n",
    "for word in unique_words:\n",
    "    if word == 'win':\n",
    "        print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second\n"
     ]
    }
   ],
   "source": [
    "word_dict = {i: word for i, word in enumerate(unique_words)}\n",
    "reversed_word_dict = {value: key for key, value in word_dict.items()}\n",
    "print(word_dict[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Edge:\n",
    "#     def __init__(self, to, weight):\n",
    "#         self.to = to\n",
    "#         self.weight = weight\n",
    "#         self.next = None\n",
    "\n",
    "\n",
    "# class Graph:\n",
    "#     def __init__(self, num_vertices):\n",
    "#         self.num_vertices = num_vertices\n",
    "#         self.adj_list = [None] * num_vertices\n",
    "\n",
    "#     def add_edge(self, u, v, weight):\n",
    "#         # 添加边 u -> v\n",
    "#         edge = Edge(v, weight)\n",
    "#         edge.next = self.adj_list[u]\n",
    "#         self.adj_list[u] = edge\n",
    "\n",
    "#     def print_graph(self):\n",
    "#         for i in range(self.num_vertices):\n",
    "#             print(\"Vertex\", i)\n",
    "#             edge = self.adj_list[i]\n",
    "#             while edge:\n",
    "#                 print(\" ->\", edge.to, \"(Weight:\", edge.weight, \")\")\n",
    "#                 edge = edge.next\n",
    "\n",
    "#     def print_node(self,i):\n",
    "#         print(\"Vertex\", i)\n",
    "#         edge = self.adj_list[i]\n",
    "#         while edge:\n",
    "#             print(\" ->\", edge.to, \"(Weight:\", edge.weight, \")\")\n",
    "#             edge = edge.next\n",
    "\n",
    "\n",
    "# num_vertices = count\n",
    "# graph = Graph(num_vertices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in word_dict:\n",
    "#     similarity = model.wv.most_similar(word,topn= 10 )\n",
    "#     for v,dis in similarity:\n",
    "#         graph.add_edge(word_dict[word],word_dict[v],dis)\n",
    "\n",
    "# graph.print_node(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self, count, word_dict, model):\n",
    "        self.count = count\n",
    "        # self.count = 6\n",
    "        self.word_dict = word_dict\n",
    "        self.model = model\n",
    "        self.graph = [[[] for _ in range(self.count)] for _ in range(self.count)]\n",
    "        self.pre = [[] for _ in range(self.count)]\n",
    "        self.dist = [0x3fffff for _ in range(self.count)]\n",
    "        self.path = []\n",
    "        self.temp_path = []\n",
    "\n",
    "    def create_graph(self):\n",
    "        for node1 in range(self.count):\n",
    "            word1 = self.word_dict[node1]\n",
    "            for node2 in range(self.count):\n",
    "                word2 = self.word_dict[node2]\n",
    "                self.graph[node1][node2] = self.get_dist(word1, word2)\n",
    "                self.graph[node2][node1] = self.graph[node1][node2]\n",
    "\n",
    "        # self.graph[9][3] = 80\n",
    "        # self.graph[3][9] = 80\n",
    "        # self.graph = [\n",
    "        #     [0, 2, 4, -1, -1, -1],\n",
    "        #     [2, 0, 1, 4, 2, -1],\n",
    "        #     [4, 1, 0, -1, 3, -1],\n",
    "        #     [-1, 4, -1, 0, 3, 2],\n",
    "        #     [-1, 2, 3, 3, 0, 2],\n",
    "        #     [-1, -1, -1, 2, 2, 0]\n",
    "        # ]\n",
    "    \n",
    "    def get_dist(self, word1, word2):\n",
    "        v1 = self.model.wv[word1]\n",
    "        v2 = self.model.wv[word2]\n",
    "        distance = np.sqrt(np.sum((v1 - v2) ** 2))\n",
    "        distance = distance ** 2   \n",
    "        return distance\n",
    "    \n",
    "    def dijkstra(self, s, t):\n",
    "        self.dist[s] = 0\n",
    "        q = PriorityQueue()\n",
    "        q.put((0, s))\n",
    "        while not q.empty():\n",
    "            dis, n = q.get()\n",
    "            if dis > self.dist[n]:\n",
    "                continue\n",
    "            for i in range(self.count):\n",
    "                if self.graph[i][n] == -1:\n",
    "                    continue\n",
    "                new_dist = self.dist[n] + self.graph[i][n]\n",
    "                if new_dist < self.dist[i]:\n",
    "                    self.dist[i] = float(new_dist)\n",
    "                    if n != i:\n",
    "                        self.pre[i] = [n]\n",
    "                    q.put((float(self.dist[i]), i))\n",
    "                elif new_dist == self.dist[i]:\n",
    "                    if n != i:\n",
    "                        self.pre[i].append(n)\n",
    "        return self.dist[t], self.pre\n",
    "\n",
    "    def record_shortest_path(self, start, target):\n",
    "        stack = LifoQueue()\n",
    "        stack.put((start, [start]))\n",
    "\n",
    "        while not stack.empty():\n",
    "            node, cur_path = stack.get()\n",
    "\n",
    "            if node == target:\n",
    "                self.path.append(cur_path)\n",
    "            for child in self.pre[node]:\n",
    "                    stack.put((child, cur_path + [child]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.45010253944736\n",
      "[[425, 768, 126, 380, 813, 914, 682, 645, 258]]\n"
     ]
    }
   ],
   "source": [
    "graph = Graph(count = count, word_dict = word_dict, model= model)\n",
    "graph.create_graph()\n",
    "\n",
    "\n",
    "def enter_word(flag):\n",
    "    while True:\n",
    "        if flag == 0:\n",
    "            str = input(\"Please enter a term: \")\n",
    "        else:\n",
    "            str = input(\"Please enter another term:\")\n",
    "        if str in reversed_word_dict:\n",
    "            return reversed_word_dict[str]\n",
    "        else:\n",
    "            print(\"The word is not in the dataset. Please enter again.\")\n",
    "\n",
    "source = enter_word(0)\n",
    "target = enter_word(1)\n",
    "# user_input_s = input(\"please input a term: \")\n",
    "# user_input_t = input(\"Please input another term: \")\n",
    "# source = reversed_word_dict[user_input_s]\n",
    "# target = reversed_word_dict[user_input_t]\n",
    "result = graph.dijkstra(source, target)\n",
    "# result = graph.dijkstra(9,3)\n",
    "print(result[0])\n",
    "graph.record_shortest_path(start= target, target= source)\n",
    "print(graph.path)\n",
    "tran_word = graph.path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal -> hit -> bar -> distance -> opportunity -> Sung-yueng -> Bony -> attacking -> kick\n"
     ]
    }
   ],
   "source": [
    "res_word = [[word_dict[num] for num in sub_path]for sub_path in tran_word]\n",
    "reversed_list = []\n",
    "for sublist in res_word:\n",
    "    reversed_sublist = sublist[::-1]\n",
    "    reversed_list.append(reversed_sublist)\n",
    "\n",
    "for sublist in reversed_list:\n",
    "    output = \" -> \".join(sublist)\n",
    "    print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
